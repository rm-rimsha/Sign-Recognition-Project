{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "433b326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d778c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD THE TRAINED MODEL\n",
    "model = keras.models.load_model(r\"C:\\Users\\RIMSHA\\ML-project\\trainedModel.h5\")\n",
    "\n",
    "#DEFINE THE BACKGROUND AND WEIGHT\n",
    "bg = None\n",
    "accumulatedWeight = 0.5\n",
    "\n",
    "#REGION OF INTEREST - DIMENSIONS\n",
    "roiTop = 100\n",
    "roiRight = 100\n",
    "roiLeft = 400\n",
    "roiBottom = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "60c91997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DETECT OBJECTS IN THE FOREGROUND:\n",
    "#CALCULATE ACCUMULATED AVERAGE FOR THE BACKGROUND\n",
    "#CALCULATE ACCUMULATED WEIGHT FOR SOME FRAMES\n",
    "#SUBTRACT THE ABOVE 2 -> FINDS ANY OBJECT\n",
    "\n",
    "def calculateAccumulatedAverage(frame, accumulatedWeight):\n",
    "    global bg\n",
    "    if bg is None:\n",
    "        bg = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    cv2.accumulateWeighted(frame, bg, accumulatedWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "772fb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handSegmentation(frame, threshold=15):\n",
    "    global bg\n",
    "    #ABSDIFF -> ABSOLUTE DIFFERENCE OF THE BACKGROUND ACCUMULATED AVERAGE AND THE ACCUMULATED WEIGHT OF FRAME\n",
    "    difference = cv2.absdiff(bg.astype(\"uint8\"), frame)\n",
    "    \n",
    "    #THRESHOLD VALUE IS DEFINED\n",
    "    #APPLY THRESHOLDING USING cv2.threshold function\n",
    "    #SIMPLE THRESHOLDING (BINARY)\n",
    "     #PIXEL VALUE < THRESHOLD => PIXEL VALUE = 0\n",
    "     #PIXEL VALUE > THRESHOLD => PIXEL VALUE = MAX (THIRD PARAMETER (255 DEFINED IN THIS))\n",
    "    ret, thresholded = cv2.threshold(difference, threshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    #CONTOURS : a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition and size estimation.\n",
    "    \n",
    "    #GET CONTOURS IN THE FRAME\n",
    "    #PARAMETERS -> thresholded image, contour retrieval model and contour approximation\n",
    "    #it returns modified image\n",
    "    #contours (list of contours -> each individual contour is numpy array)\n",
    "    \n",
    "    #NEXT \n",
    "    #the contour's length is checked to see if its a hand or not\n",
    "    \n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    #no contours\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "    #hand contour would be the maximum one\n",
    "        segmentedHandMaxContour = max(contours, key=cv2.contourArea)\n",
    "        return (thresholded, segmentedHandMaxContour)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed928b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 265ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 219ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n"
     ]
    }
   ],
   "source": [
    "#labelNames = {0:'Zero', 1:'One', 2:'two', 3:'Three', 4:'Four', 5:'Five', 6:'Six', 7:'Seven', 8:'Eight', 9:'Nine'}\n",
    "\n",
    "labelNames = {0:'Zero', 1:'One', 2:'two', 3:'Three', 4:'Four', 5:'Five', 6:'Six', 7:'Seven', 8:'Eight', 9:'Nine',\n",
    "              10: 'a', 11:'b', 12:'c', 13:'d', 14:'e', 15:'f', 16:'g', 17:'h', 18:'i', 19:'j', 20:'k',\n",
    "              21: 'l', 22:'m', 23:'n', 24:'o', 25:'p', 26:'q', 27:'r', 28:'s', 29:'t', 30:'u', 31:'v',\n",
    "              32: 'w', 33:'x', 34:'y', 35:'z' }\n",
    "\n",
    "#HAND DETECTION ON LIVE CAMERA\n",
    "cam = cv2.VideoCapture(0)\n",
    "numOfFrames = 0\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    \n",
    "    #flip the frame for the inverted image captured\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    frameCopy = frame.copy()\n",
    "    \n",
    "    #REGION OF INTEREST\n",
    "    roi = frame[roiTop: roiBottom, roiRight: roiLeft]\n",
    "    \n",
    "    #CONVERT THE ROI FRAME INTO A GRAY FRAME THEN APPLY GAUSSIAN BLUR\n",
    "    #TO GET B/W IMAGES\n",
    "    grayF = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if(numOfFrames < 70):\n",
    "        calculateAccumulatedAverage(grayF, accumulatedWeight)\n",
    "        cv2.putText(frameCopy, \"FETCHING BACKGROUND...PLEASE WAIT\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "    else:\n",
    "        hand = handSegmentation(grayF)\n",
    "        \n",
    "        if hand is not None:\n",
    "            thresholded, segmentedHand = hand\n",
    "            cv2.drawContours(frameCopy, [segmentedHand + (roiRight, roiTop)], -1, (255,0,0), 1)\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            \n",
    "            thresholded = cv2.resize(thresholded, (128, 128))\n",
    "            thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "            thresholded = np.reshape(thresholded, (1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "            \n",
    "            prediction = model.predict(thresholded)\n",
    "            cv2.putText(frameCopy, labelNames[np.argmax(prediction)], (170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "    cv2.rectangle(frameCopy, (roiLeft, roiTop), (roiRight, roiBottom), (255,128,0), 3)\n",
    "    numOfFrames += 1\n",
    "    cv2.putText(frameCopy, \"Sign Recognition\" , (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "    cv2.imshow(\"Sign detection\", frameCopy)\n",
    "    \n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79940a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a4258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a01ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
